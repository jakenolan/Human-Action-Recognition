{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Human Action Recognition (HAR)**"
      ],
      "metadata": {
        "id": "qW78o8M5wuet"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u96Rzi2YZi6u",
        "outputId": "3eeab415-e52e-40a8-debc-68094ff9fd37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▌                             | 10 kB 35.6 MB/s eta 0:00:01\r\u001b[K     |█████                           | 20 kB 42.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30 kB 42.8 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 40 kB 31.2 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 51 kB 34.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 61 kB 39.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 71 kB 27.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 81 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 92 kB 30.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 102 kB 32.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 112 kB 32.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 122 kB 32.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 133 kB 32.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133 kB 32.6 MB/s \n",
            "\u001b[?25hEverything successfully imported!\n"
          ]
        }
      ],
      "source": [
        "# Set working environment variable (keep one true)\n",
        "my_machine = False\n",
        "colab = True\n",
        "# Set TPU bool (can be used in colab)\n",
        "# Current versioning error using a TPU so keep false (to be fixed)\n",
        "tpu = False\n",
        "\n",
        "# Assumed packages installed on local machine\n",
        "# If colab install keras tuner (not included in colab)\n",
        "if colab:\n",
        "    %pip install -q -U keras-tuner\n",
        "\n",
        "# Major imports\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib as mp\n",
        "import keras_tuner\n",
        "\n",
        "# Partial imports\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Confirm completion\n",
        "print( \"Everything successfully imported!\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpWLAd0PZi6y"
      },
      "outputs": [],
      "source": [
        "# If on your machine check for GPU and and enable memory growth\n",
        "# Do not need to run if not using a GPU\n",
        "if not tpu:\n",
        "    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if not physical_devices:\n",
        "        print( \"No GPU recognized!\" )\n",
        "    else:\n",
        "        print( \"Number of GPUs recognized: \", len(physical_devices) )\n",
        "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "# If using Colab you can connect and init a TPU for training\n",
        "if tpu and colab:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "    tpu_stat = tf.distribute.experimental.TPUStrategy(tpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SljXq3f5Zi6z"
      },
      "outputs": [],
      "source": [
        "# Collect train and test data from csv files\n",
        "# HAR data found on Kaggle\n",
        "if my_machine:\n",
        "    ROOT_DIR = os.getcwd()\n",
        "    train_set_csv = pd.read_csv( os.path.join(ROOT_DIR, \"Training_set.csv\") )\n",
        "    test_set_csv = pd.read_csv( os.path.join(ROOT_DIR, \"Testing_set.csv\") )\n",
        "\n",
        "if colab:\n",
        "    from google.colab import files\n",
        "    upload = files.upload()\n",
        "    train_set_csv = pd.read_csv('Training_set.csv')\n",
        "    test_set_csv = pd.read_csv('Testing_set.csv')\n",
        "\n",
        "num_classes = len(train_set_csv['label'].unique())\n",
        "print( \"Total number of action classes: \", num_classes )\n",
        "print( \"List of action classes: \" )\n",
        "print( train_set_csv['label'].unique() )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYUiJHIiZi6z"
      },
      "outputs": [],
      "source": [
        "# Get path to or upload train and test data\n",
        "if my_machine:\n",
        "    TRAIN_DIR = os.path.join(ROOT_DIR, \"train\")\n",
        "    TEST_DIR = os.path.join(ROOT_DIR, \"test\")\n",
        "if colab:\n",
        "    from google.colab import files\n",
        "    upload = files.upload()\n",
        "    with zipfile.ZipFile(\"train.zip\", \"r\") as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "    TRAIN_DIR = \"train\"\n",
        "    with zipfile.ZipFile(\"test.zip\", \"r\") as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "    TEST_DIR = \"test\"\n",
        "\n",
        "# Create train dataset and apply transformations\n",
        "train_image_generator = ImageDataGenerator(\n",
        "    rescale=1./255, \n",
        "    horizontal_flip=True, \n",
        "    shear_range=0.2, \n",
        "    zoom_range=0.2\n",
        ")\n",
        "train_dataset = train_image_generator.flow_from_dataframe(\n",
        "    dataframe=train_set_csv,\n",
        "    directory=TRAIN_DIR,\n",
        "    x_col='filename',\n",
        "    y_col='label',\n",
        "    target_size=(224, 224),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Create test dataset and apply transformations\n",
        "test_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "test_dataset = test_image_generator.flow_from_dataframe(\n",
        "    dataframe=test_set_csv, \n",
        "    directory=TEST_DIR,\n",
        "    x_col='filename',\n",
        "    y_col=None,\n",
        "    target_size=(224, 224),\n",
        "    class_mode=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boaLuMAJZi60"
      },
      "outputs": [],
      "source": [
        "def build_cnn_model():\n",
        "    # Create basic CNN Model\n",
        "    cnn = Sequential()\n",
        "    cnn.add( Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[224, 224, 3]) )\n",
        "    cnn.add( MaxPool2D(pool_size=2, strides=2) )\n",
        "    cnn.add( Conv2D(filters=32, kernel_size=3, activation='relu') )\n",
        "    cnn.add( MaxPool2D(pool_size=2, strides=2) )\n",
        "    cnn.add( Flatten() )\n",
        "    cnn.add( Dense(units=448, activation='relu') )\n",
        "    cnn.add( Dense(units=num_classes, activation='sigmoid') )\n",
        "    # Compile modile and return\n",
        "    cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return cnn\n",
        "\n",
        "# Check for tpu and then create model (avoids rewriting the model)\n",
        "if not tpu:\n",
        "    cnn = build_cnn_model()\n",
        "if tpu and colab:\n",
        "    with tpu_stat.scope():\n",
        "        cnn = build_cnn_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMAX_4QjZi61"
      },
      "outputs": [],
      "source": [
        "# Fit basic model and save weights\n",
        "cnn.fit(x=train_dataset, validation_data=test_dataset, epochs=10)\n",
        "cnn.save_weights(\"cnn_har.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zFieFeBZi61"
      },
      "outputs": [],
      "source": [
        "# ------------------------------ WORK IN PROGRESS ------------------------------\n",
        "def build_vgg_model(hp):\n",
        "    # Create base cnn layers from VGG16 but cut off dense layers to use our own instead\n",
        "    # Use imagenet weights and lock them to be untrainable\n",
        "    vgg = Sequential()\n",
        "    pretrained_model= tf.keras.applications.VGG16(include_top=False, input_shape=(224, 224, 3), pooling='max', weights='imagenet')\n",
        "    for layer in pretrained_model.layers:\n",
        "            layer.trainable=False\n",
        "    # Add dense layers to VGG16 for our personal use with 15 classes output\n",
        "    # Use keras tuner for number of internal dense layers and number of nodes for each dense layer\n",
        "    vgg.add( pretrained_model )\n",
        "    vgg.add( Flatten() )\n",
        "    for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
        "        vgg.add( Dense(hp.Int(\"nodes_{i}\", min_value=32, max_value=512, step=32), activation='relu') )\n",
        "    # Output layer\n",
        "    vgg.add( Dense(15, activation='softmax') )\n",
        "    # Compile model and return\n",
        "    vgg.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return vgg\n",
        "\n",
        "# Check for tpu and then create model (avoids rewriting the model)\n",
        "if not tpu:\n",
        "    vgg = build_vgg_model(keras_tuner.HyperParameters())\n",
        "if tpu and colab:\n",
        "    with tpu_stat.scope():\n",
        "        vgg = build_vgg_model(keras_tuner.HyperParameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61Ze7baOaMQC"
      },
      "outputs": [],
      "source": [
        "# ------------------------------ WORK IN PROGRESS ------------------------------\n",
        "# Define tuner search and check summary\n",
        "tuner = keras_tuner.RandomSearch(\n",
        "    hypermodel=build_vgg_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    overwrite=True,\n",
        "    directory=\"/\",\n",
        "    project_name=\"vgg_har_model\",\n",
        ")\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------ WORK IN PROGRESS ------------------------------\n",
        "# Start tuner search for optimal model\n",
        "tuner.search(x=train_dataset, validation_data=test_dataset, epochs=3)"
      ],
      "metadata": {
        "id": "2ZR1TxH94feP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLF55OQ5Zi62"
      },
      "outputs": [],
      "source": [
        "# ------------------------------ WORK IN PROGRESS ------------------------------\n",
        "# Fit model and save resulting weights\n",
        "vgg.fit(x=train_dataset, validation_data=test_dataset, epochs=50)\n",
        "vgg.save_weights(\"vgg_har.h5\")\n",
        "# If using Colab it will save weights to local machine\n",
        "if colab:\n",
        "    from google.colab import files\n",
        "    files.download(\"vgg_har.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt0tBLY7Zi63"
      },
      "outputs": [],
      "source": [
        "# Create labels dict for reference\n",
        "labels_ref = (train_dataset.class_indices)\n",
        "labels = {}\n",
        "for name, index in labels_ref.items():\n",
        "    labels[index] = name\n",
        "\n",
        "# Print labels dictionary evenly for a visual\n",
        "print( \"------------------------------\" )\n",
        "print( \"Labels: \" )\n",
        "for index in range(len(labels)):\n",
        "    if index < 10:\n",
        "        print( \" \" + str(index) + \": \" + labels[index] )\n",
        "    else:\n",
        "        print( str(index) + \": \" + labels[index] )\n",
        "\n",
        "# Collect 5 random pictures from test files and run predictions\n",
        "test_files = os.listdir(TEST_DIR)\n",
        "random_numbers = np.random.randint(low=0, high=len(test_files), size=5)\n",
        "for i in random_numbers:\n",
        "    # Collect test file and handle image\n",
        "    test_file = TEST_DIR + \"/\" + test_files[i]\n",
        "    test_image = keras.preprocessing.image.load_img(test_file, target_size=(224, 224))\n",
        "    test_image = keras.preprocessing.image.img_to_array(test_image)\n",
        "    test_image = np.expand_dims(test_image, axis=0)\n",
        "    test_file = mp.image.imread(test_file)\n",
        "    plot = mp.pyplot.imshow(test_file)\n",
        "    print( \"------------------------------\" )\n",
        "    print( \"\\n\" )\n",
        "    print( test_files[i] )\n",
        "    print( \"\\n\" )\n",
        "    mp.pyplot.show()\n",
        "    print( \"\\n\" )\n",
        "    # Run predictions from tested model\n",
        "    #prediction = cnn.predict([[test_image]])\n",
        "    prediction = vgg.predict([[test_image]])\n",
        "    results = {}\n",
        "    for index in range(len(prediction[0])):\n",
        "        results[index] = (prediction[0][index])\n",
        "    sorted_results = sorted(results, key=results.get, reverse=True)\n",
        "    for index in sorted_results:\n",
        "        print( labels[index] + \": \" + str(results[index]) )\n",
        "        print( \"\\n\" )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "cv_for_har.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 ('cvenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "b85e13c92cb50cc8954c2f15a1ff4a46104e3ef963a141e57ec86a5992533f71"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}